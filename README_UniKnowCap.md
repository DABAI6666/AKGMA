To evaluate different methods' ability to integrate knowledge and perform knowledge reasoning in image captioning, a dataset with knowledge-enhanced captions is required. Standard image captioning datasets, such as MSCOCO, Flickr, and nocaps, contain images of common objects but include limited real-world knowledge, such as named entities. Some datasets focus on specific knowledge, but they are often domain-specific, such as geographic or news knowledge. More importantly, these tasks require additional image metadata, making them unsuitable for evaluating the generalizability of models generating knowledge-enhanced image descriptions. While the KnowCap dataset measures knowledge recognition accuracy, it mainly covers simple domain scenes, such as landmarks, famous brands, special foods, and movie characters, limiting the evaluation of large models in broader, more complex scenarios and their susceptibility to knowledge hallucination.

To address this gap, we introduce **UniKnowCap**, a new dataset designed to evaluate the generalization of knowledge-enhanced image captioning models across a broader and more practical range of knowledge domains, such as science and technology, medicine and health, culture and arts, and history and geography. To collect images containing diverse knowledge types, we first guided ChatGPT to generate 480 relevant keyword combinations, covering a wide array of topics. We then crawled over 40,000 images from the internet using these keywords. After filtering by five expert annotators, **3,074** images  (the article mistakenly stated 2,074; this will be corrected in the final version) suitable for the image captioning task were selected (e.g., images containing multiple objects or complex scenes). For each image, five carefully written reference descriptions were collected, resulting in 14,983 reference captions. Over 70% of these descriptions involve entity interactions or cultural/artistic contexts, placing higher demands on the model's knowledge reasoning abilities.